{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install apache_beam --quiet"
      ],
      "metadata": {
        "id": "IOZRW5Q_Y4qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h8hCAiqBYmxJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'XXX' #specify your json credentials"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "bucket_name = 'fh-public'  # Replace with your bucket name\n",
        "prefix = 'wikicrow2/'\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(bucket_name)"
      ],
      "metadata": {
        "id": "hnqL6QILYu6W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blobs = client.list_blobs(bucket, prefix=prefix)\n",
        "data_files = [blob.name for blob in blobs ]"
      ],
      "metadata": {
        "id": "ej-bd01NYwBb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Define a local directory to store downloaded data\n",
        "download_dir = 'wikicrow2'\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "def download_file(file_name):\n",
        "    \"\"\"\n",
        "    Downloads a single file from the bucket to the local directory.\n",
        "\n",
        "    Args:\n",
        "        file_name (str): The name/path of the file in the bucket.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the downloaded file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        blob = bucket.blob(file_name)\n",
        "        local_path = os.path.join(download_dir, os.path.basename(file_name))\n",
        "\n",
        "        # Download the file\n",
        "        blob.download_to_filename(local_path)\n",
        "        print(f\"Downloaded {file_name} to {local_path}\")\n",
        "        return file_name\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {file_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Specify the number of threads.\n",
        "# You can adjust this number based on your system's capabilities and network bandwidth.\n",
        "MAX_WORKERS = 10\n",
        "\n",
        "# Use ThreadPoolExecutor to manage concurrent downloads\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    # Submit all download tasks to the executor\n",
        "    future_to_file = {executor.submit(download_file, file_name): file_name for file_name in data_files}\n",
        "\n",
        "    # As each task completes, handle the result\n",
        "    for future in as_completed(future_to_file):\n",
        "        file_name = future_to_file[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                # Optional: Additional processing can be done here\n",
        "                pass\n",
        "        except Exception as exc:\n",
        "            print(f\"{file_name} generated an exception: {exc}\")\n",
        "\n",
        "print(\"All downloads completed.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1FHEhviOYxE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJb-kd-TYxuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_keys = [file.replace('.txt', '') for file in data_files if file.endswith('.txt')]\n",
        "file_keys = [file.replace('wikicrow2/', '') for file in file_keys]"
      ],
      "metadata": {
        "id": "DTaqf31VYzNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGk80AJRYzfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import requests\n",
        "import os\n",
        "import sqlite3\n",
        "import threading\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "# Function to map gene names to UniProt IDs\n",
        "def map_gene_to_uniprot(gene_name, organism_id=9606):\n",
        "    base_url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
        "    query = f\"gene_exact:{gene_name} AND organism_id:{organism_id}\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"format\": \"json\",\n",
        "        \"fields\": \"accession\",\n",
        "        \"size\": 1  # Only top result\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data['results']:\n",
        "            uniprot_id = data['results'][0]['primaryAccession']\n",
        "            return (gene_name, uniprot_id)\n",
        "        else:\n",
        "            tqdm.write(f\"No UniProt accession found for gene '{gene_name}'.\")\n",
        "            return (gene_name, None)\n",
        "\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        tqdm.write(f\"HTTP error for gene '{gene_name}': {http_err}\")\n",
        "        return (gene_name, None)\n",
        "    except Exception as err:\n",
        "        tqdm.write(f\"Error for gene '{gene_name}': {err}\")\n",
        "        return (gene_name, None)\n",
        "\n",
        "# Function to construct AlphaFold PDB URL\n",
        "def construct_alphafold_pdb_url(uniprot_id, model, version):\n",
        "    base_url = \"https://alphafold.ebi.ac.uk/files\"\n",
        "    pdb_url = f\"{base_url}/AF-{uniprot_id}-{model}-model_v{version}.pdb\"\n",
        "    return pdb_url\n",
        "\n",
        "# Function to download PDB files\n",
        "def download_pdb(gene_uniprot_tuple, output_dir='pdb_files'):\n",
        "    gene, uniprot_id = gene_uniprot_tuple\n",
        "\n",
        "    if not uniprot_id:\n",
        "        tqdm.write(f\"Skipping download for gene '{gene}' due to missing UniProt ID.\")\n",
        "        return (gene, None)\n",
        "\n",
        "    versions = ['4', '3', '2', '1']\n",
        "    models = ['F1', 'F2', 'F3', 'F4', 'F5']\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for version in versions:\n",
        "        for model in models:\n",
        "            pdb_url = construct_alphafold_pdb_url(uniprot_id, model, version)\n",
        "            filename = f\"AF-{uniprot_id}-{model}-model_v{version}.pdb\"\n",
        "            save_path = os.path.join(output_dir, filename)\n",
        "\n",
        "            try:\n",
        "                response = requests.get(pdb_url, stream=True)\n",
        "                if response.status_code == 200:\n",
        "                    with open(save_path, 'wb') as file:\n",
        "                        for data in response.iter_content(1024):\n",
        "                            file.write(data)\n",
        "                    return (gene, os.path.abspath(save_path))  # Return absolute path\n",
        "                else:\n",
        "                    continue\n",
        "            except Exception as err:\n",
        "                tqdm.write(f\"Error downloading {filename} for UniProt ID '{uniprot_id}': {err}\")\n",
        "                continue\n",
        "\n",
        "    tqdm.write(f\"No PDB files found for UniProt ID '{uniprot_id}' for gene '{gene}'.\")\n",
        "    return (gene, None)\n",
        "\n",
        "# Lock for database access\n",
        "db_lock = threading.Lock()\n",
        "\n",
        "def process_gene(x_and_y):\n",
        "    x, y = x_and_y\n",
        "    try:\n",
        "        with open(y, \"r\") as f:\n",
        "            description = f.read()\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"Error reading file {y}: {e}\")\n",
        "        description = \"\"\n",
        "\n",
        "    gene_name = x\n",
        "    gene_uniprot_tuple = map_gene_to_uniprot(gene_name)\n",
        "    download_pdb(gene_uniprot_tuple)\n",
        "    gene, uniprot_id = gene_uniprot_tuple\n",
        "    gene_pdb_tuple = (gene_uniprot_tuple)\n",
        "    gene, pdb_file_path = gene_pdb_tuple\n",
        "\n",
        "    # Insert into SQL database\n",
        "    with db_lock:\n",
        "        cursor.execute('''\n",
        "            INSERT INTO genes (gene_name, description, uniprot_id, pdb_file)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        ''', (gene_name, description, uniprot_id, pdb_file_path))\n",
        "        conn.commit()\n",
        "\n",
        "    return (gene_name, description, uniprot_id, pdb_file_path)\n",
        "\n",
        "# Your lists of gene names and data file\n",
        "\n",
        "# Create the database connection\n",
        "conn = sqlite3.connect('genes.db', check_same_thread=False)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create the table\n",
        "cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS genes (\n",
        "        gene_name TEXT,\n",
        "        description TEXT,\n",
        "        uniprot_id TEXT,\n",
        "        pdb_file TEXT\n",
        "    )\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "# Use ThreadPoolExecutor for concurrency\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Wrap the executor.map call with tqdm for progress bar\n",
        "    results = list(tqdm(\n",
        "        executor.map(process_gene, zip(file_keys, data_files)),\n",
        "        total=len(file_keys),\n",
        "        desc=\"Processing Genes\",\n",
        "        unit=\"gene\"\n",
        "    ))\n",
        "\n",
        "# Close the database connection\n",
        "conn.close()\n",
        "\n",
        "print(\"Processing complete. Data stored in 'genes.db' database.\")\n"
      ],
      "metadata": {
        "id": "2E_h6qRGYz2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vEwDxizY1Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import apache_beam as beam\n",
        "from google.cloud import storage\n",
        "\n",
        "class UploadToGCS(beam.DoFn):\n",
        "    def __init__(self, bucket_name, destination_path):\n",
        "        self.bucket_name = bucket_name\n",
        "        self.destination_path = destination_path\n",
        "        self.client = None\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the GCS client once per worker\n",
        "        self.client = storage.Client()\n",
        "\n",
        "    def process(self, file_path):\n",
        "        # Extract the file name\n",
        "        file_name = os.path.basename(file_path)\n",
        "        # Build the GCS destination path\n",
        "        gcs_blob_path = f\"{self.destination_path}/{file_name}\"\n",
        "        # Upload the file to GCS\n",
        "        bucket = self.client.bucket(self.bucket_name)\n",
        "        blob = bucket.blob(gcs_blob_path)\n",
        "        blob.upload_from_filename(file_path)\n",
        "        yield f\"Uploaded {file_path} to gs://{self.bucket_name}/{gcs_blob_path}\"\n",
        "\n",
        "\n",
        "# Configuration\n",
        "bucket_name = \"xxxx\"  # Replace with your GCS bucket name\n",
        "destination_path = \"pdb_files\"  # The target folder in GCS\n",
        "local_folder = \"/content/pdb_files\"  # Folder containing your .pdb files\n",
        "\n",
        "# List all files in the local folder\n",
        "def list_files(folder):\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            yield os.path.join(root, file)\n",
        "\n",
        "\n",
        "# Apache Beam Pipeline\n",
        "with beam.Pipeline() as pipeline:\n",
        "    (\n",
        "        pipeline\n",
        "        | \"List Files\" >> beam.Create(list_files(local_folder))  # Step 1: List all files\n",
        "        | \"Upload Files\" >> beam.ParDo(UploadToGCS(bucket_name, destination_path))  # Step 2: Upload files\n",
        "        | \"Print Results\" >> beam.Map(print)  # Step 3: Print results\n",
        "    )\n"
      ],
      "metadata": {
        "id": "0yxpo3wIY1NZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}